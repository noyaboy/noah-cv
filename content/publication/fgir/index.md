---
title: "Fine-Grained Image Recognition from Scratch with Teacher-Guided Data Augmentation"
authors:
  - Edwin Arkel Rios
  - Fernando Mikael
  - Oswin Gosal
  - Femiloye Oyerinde
  - Hao-Chun Liang
  - Bo-Cheng Lai
  - Min-Chun Hu
date: "2025-07-16T00:00:00Z"
publishDate: "2025-07-16T00:00:00Z"
doi: ""
publication_types: ["journal-paper"]
publication: "Arxiv"
publication_short: ""
abstract: >
  Fine‑grained image recognition no longer needs ImageNet pretraining. TGDA trains models from scratch by mixing data‑aware augmentation with weakly supervised knowledge distillation from a fine‑grained teacher. This frees designers to build task‑ and hardware‑specific networks: LRNets excel on low‑resolution inputs, while ViTFS Transformers offer efficient high‑res inference. Across three FGIR benchmarks, TGDA‑trained models equal or beat state‑of‑the‑art pretrained systems. LRNets gain up to 23 % accuracy using 20.6 × fewer parameters and far less data; ViTFS‑T matches ViT‑B/16 pretrained on ImageNet‑21k with 15.3 × fewer trainable parameters.
summary:
tags: [FGIR, TGDA, Training from Scratch, LRNet, ViTFS]
featured: true
url_pdf: uploads/fgir.pdf
url_code: ""
url_dataset: ""
url_poster: ""
url_project: ""
url_slides: ""
url_source: ""
url_video: ""
image:
  caption: "Overview of the TGDA Framework"
  focal_point: ""
  preview_only: false
projects: []
slides: ""
---